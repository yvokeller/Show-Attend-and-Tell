{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "My goal with this notebook is to show the direct relation between the paper (Show, Attend and Tell) and it's implementation within this repository. I will try to explain the code as much as possible, nontheless recommend you to read the paper first.\n",
    "\n",
    "The implementation is based on the [AaronCCWong's implementation](https://github.com/AaronCCWong/Show-Attend-and-Tell). I build on top of his work, trying some alternative techniques (like using pre-trained embeddings) and more closely aligning it with the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3.1.1 in the \"Show, Attend and Tell\" paper describes the encoder. The proposed model uses a CNN to extract a set of feature vectors, referred to as annotation vectors. This code implements that concept by using pre-trained models (VGG19, ResNet152, or DenseNet161), modifying them to exclude the final classification layers, and reshaping the output to form a set of feature vectors (our annotation vectors). Each vector represents different spacial parts of the image, which is key for the attention mechanism in the next stages of the model.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from torchvision.models import densenet161, resnet152, vgg19\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network for image feature extraction, follows section 3.1.1 of the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, network='vgg19'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.network = network\n",
    "        # Selection of pre-trained CNNs for feature extraction\n",
    "        if network == 'resnet152':\n",
    "            self.net = resnet152(pretrained=True)\n",
    "            # Removing the final fully connected layers of ResNet152\n",
    "            self.net = nn.Sequential(*list(self.net.children())[:-2])\n",
    "            self.dim = 2048  # Dimension of feature vectors for ResNet152\n",
    "        elif network == 'densenet161':\n",
    "            self.net = densenet161(pretrained=True)\n",
    "            # Removing the final layers of DenseNet161\n",
    "            self.net = nn.Sequential(*list(list(self.net.children())[0])[:-1])\n",
    "            self.dim = 1920  # Dimension of feature vectors for DenseNet161\n",
    "        else:\n",
    "            self.net = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "            # Using features from VGG19, excluding the last pooling layer\n",
    "            self.net = nn.Sequential(*list(self.net.features.children())[:-1])\n",
    "            self.dim = 512  # Dimension of feature vectors for VGG19\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # These steps correspond to the extraction of annotation vectors (a = {a1,...,aL}) as described in Section 3.1.1 of the paper.\n",
    "        # 1. Change the order from (BS, C, H, W) to (BS, H, W, C) in prep for reshaping\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # 2. Reshape to [BS, num_spatial_features, C], the -1 effectively flattens the height and width dimensions into a single dimension\n",
    "        x = x.view(x.size(0), -1, x.size(-1))\n",
    "        return x\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "\n",
    "Let's move forward to Section 3.1.2, explaining the Decoder. The Decoder uses the extracted image features to initialize the states of the LSTM cell (by averaging them) and then employs an **attention mechanism** at each time step to focus on different parts of the image while generating the caption. The Decoder predicts one word of the caption at each time step, and its prediction is conditioned on the current LSTM state, the context vector from the attention mechanism, and the previous word. Teacher forcing, a common technique in training sequence generation models where the ground truth word is fed as the next input instead of the model's prediction, is optionally used.\n",
    "\n",
    "Provided below is the full implementation of the Decoder. I will break the code down into smaller chunks to explore it in more detail, based on the paper's description of the Decoder.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import Attention\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocabulary_size, encoder_dim, tf=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_tf = tf\n",
    "\n",
    "        # Initializing parameters\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "        # Initial LSTM cell state generators\n",
    "        self.init_h = nn.Linear(encoder_dim, 512)  # For hidden state\n",
    "        self.init_c = nn.Linear(encoder_dim, 512)  # For cell state\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Attention mechanism related layers\n",
    "        self.f_beta = nn.Linear(512, encoder_dim)  # Gating scalar in attention mechanism\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Output layer and embedding\n",
    "        self.deep_output = nn.Linear(512, vocabulary_size)  # Maps LSTM outputs to vocabulary\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        # Attention and LSTM components\n",
    "        self.attention = Attention(encoder_dim)  # Attention network\n",
    "        self.embedding = nn.Embedding(vocabulary_size, 512)  # Embedding layer for input words\n",
    "        self.lstm = nn.LSTMCell(512 + encoder_dim, 512)  # LSTM cell\n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        # Forward pass of the decoder\n",
    "        batch_size = img_features.size(0)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.get_init_lstm_state(img_features)\n",
    "\n",
    "        # Teacher forcing setup\n",
    "        max_timespan = max([len(caption) for caption in captions]) - 1\n",
    "        prev_words = torch.zeros(batch_size, 1).long().to(mps_device)\n",
    "        if self.use_tf:\n",
    "            embedding = self.embedding(captions) if self.training else self.embedding(prev_words)\n",
    "        else:\n",
    "            embedding = self.embedding(prev_words)\n",
    "\n",
    "        # Preparing to store predictions and attention weights\n",
    "        preds = torch.zeros(batch_size, max_timespan, self.vocabulary_size).to(mps_device)\n",
    "        alphas = torch.zeros(batch_size, max_timespan, img_features.size(1)).to(mps_device)\n",
    "\n",
    "        # Generating captions\n",
    "        for t in range(max_timespan):\n",
    "            context, alpha = self.attention(img_features, h)  # Compute context vector via attention\n",
    "            gate = self.sigmoid(self.f_beta(h))  # Gating scalar for context\n",
    "            gated_context = gate * context  # Apply gate to context\n",
    "\n",
    "            # Prepare LSTM input\n",
    "            if self.use_tf and self.training:\n",
    "                lstm_input = torch.cat((embedding[:, t], gated_context), dim=1)\n",
    "            else:\n",
    "                embedding = embedding.squeeze(1) if embedding.dim() == 3 else embedding\n",
    "                lstm_input = torch.cat((embedding, gated_context), dim=1)\n",
    "\n",
    "            # LSTM forward pass\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            output = self.deep_output(self.dropout(h))  # Generate word prediction\n",
    "\n",
    "            preds[:, t] = output\n",
    "            alphas[:, t] = alpha  # Store attention weights\n",
    "\n",
    "            # Prepare next input word\n",
    "            if not self.training or not self.use_tf:\n",
    "                embedding = self.embedding(output.max(1)[1].reshape(batch_size, 1))\n",
    "        return preds, alphas\n",
    "\n",
    "    def get_init_lstm_state(self, img_features):\n",
    "        # Initializing LSTM state based on image features\n",
    "        avg_features = img_features.mean(dim=1)\n",
    "\n",
    "        c = self.init_c(avg_features)  # Cell state\n",
    "        c = self.tanh(c)\n",
    "\n",
    "        h = self.init_h(avg_features)  # Hidden state\n",
    "        h = self.tanh(h)\n",
    "\n",
    "        return h, c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A close look at the deep_output layer\n",
    "\n",
    "Show, Attend and Tell utilizes a *deep output layer* (Pascanu et al., 2014) to compute the output word probability given given the current state of the LSTM, the context vector from the attention mechanism, and the previously generated word.\n",
    "\n",
    "Let's break down this formula and map its components to the code:\n",
    "\n",
    "$$ p\\left(\\mathbf{y}_t \\mid \\mathbf{a}, \\mathbf{y}_1^{t-1}\\right) \\propto \\exp \\left(\\mathbf{L}_o\\left(\\mathbf{E} \\mathbf{y}_{t-1}+\\mathbf{L}_h \\mathbf{h}_t+\\mathbf{L}_z \\hat{\\mathbf{z}}_t\\right)\\right) $$\n",
    "\n",
    "\n",
    "\n",
    "Where p of $ \\mathbf{y}_t $ is the probability of the output word $ y $ at time $ _t $ given the image features $ \\mathbf{a} $ and the previously generated words $ \\mathbf{y}_1^{t-1} $.\n",
    "\n",
    "In this formula:\n",
    "\n",
    "- $ \\mathbf{y}_t $ is the output word at time $ t $.\n",
    "- $ \\mathbf{a} $ represents the set of annotation vectors (image features).\n",
    "- $ \\mathbf{y}_1^{t-1} $ are the previously generated words up to time $ t-1 $.\n",
    "- $ \\mathbf{L}_o, \\mathbf{L}_h, \\mathbf{L}_z $ are learned weight matrices.\n",
    "- $ \\mathbf{E} $ is the embedding matrix for the previous word $ \\mathbf{y}_{t-1} $.\n",
    "- $ \\mathbf{h}_t $ is the hidden state of the LSTM at time $ t $.\n",
    "- $ \\hat{\\mathbf{z}}_t $ is the context vector at time $ t $, generated by the attention mechanism.\n",
    "\n",
    "Now let's map this to the Decoder's code:\n",
    "\n",
    "1. **Embedding of the Previous Word ($ \\mathbf{E} \\mathbf{y}_{t-1} $)**: This is done using the `self.embedding` layer in the code.\n",
    "\n",
    "    ```python\n",
    "    embedding = self.embedding(prev_words)\n",
    "    ```\n",
    "\n",
    "2. **Hidden State of the LSTM ($ \\mathbf{h}_t $)**: The `h` variable in the code represents the hidden state of the LSTM at each time step.\n",
    "\n",
    "    ```python\n",
    "    h, c = self.lstm(lstm_input, (h, c))\n",
    "    ```\n",
    "\n",
    "3. **Context Vector ($ \\hat{\\mathbf{z}}_t $)**: The context vector is computed by the attention mechanism in the `self.attention` layer.\n",
    "\n",
    "    ```python\n",
    "    context, alpha = self.attention(img_features, h)\n",
    "    ```\n",
    "\n",
    "4. **Combining and Transforming for Output Prediction**: The output word probability is computed by combining these elements and applying the learned weight matrices. In the code, this operation is currently condensed into one `self.deep_output` layer transforming the hidden state $ \\mathbf{h}_t $. In a more complex or literal implementation of the DO-RNN, you would expect to see multiple such layers, each followed by a non-linear activation function.\n",
    "\n",
    "    ```python\n",
    "    output = self.deep_output(self.dropout(h))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "At the heart of the Show, Attend and Tell model is the attention mechanism. The attention mechanism is used to focus on different parts of the image while generating the caption. The attention mechanism is implemented as a separate module, which is used by the Decoder at each time step.\n",
    "\n",
    "There are two main types of attention mechanisms: **soft attention** and **hard attention**. Soft attention is differentiable and allows for end-to-end training, while hard attention is non-differentiable and requires reinforcement learning to train. I want to break down both types of attention mechanisms theoretically and then show how they are implemented in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Hard Attention\n",
    "\n",
    "Stochastic \"Hard\" Attention is an approach where the model discretely chooses specific regions (or locations, i.e. one annotation vector) in an image to focus on at each step of generating a caption. This contrasts with \"Soft\" Attention, where the model considers all regions but with varying degrees of focus.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Attention Location Representation\n",
    "\n",
    "$ s_{t, i} = 1 $ if the $ i $-th location is chosen at time $ t $, out of $ L $ total locations.\n",
    "\n",
    "**Significance**: Represents the model's decision on where to focus in the image when generating the $ t $-th word in the caption as a one-hot vector.\n",
    "\n",
    "#### 2. Context Vector Computation\n",
    "\n",
    "$$ \\hat{\\mathbf{z}}_t = \\sum_i s_{t, i} \\mathbf{a}_i $$\n",
    "\n",
    "**Significance**: Computes the context vector as the feature vector of the selected image region. Only the chosen region contributes to the context at each step.\n",
    "\n",
    "#### 3. Attention as Multinoulli Distribution\n",
    "\n",
    "$$ \\tilde{s}_t \\sim \\operatorname{Multinoulli}_L(\\{\\alpha_i\\}) $$\n",
    "\n",
    "**Significance**: Models the attention decision as a random variable, following a Multinoulli distribution. The attention weights $ \\{\\alpha_i\\} $ determine the probability of focusing on each region.\n",
    "\n",
    "#### 4. Objective Function (Variational Lower Bound)\n",
    "\n",
    "$$ L_s = \\sum_s p(s | \\mathbf{a}) \\log p(\\mathbf{y} | s, \\mathbf{a}) $$\n",
    "$$ L_s \\leq \\log p(\\mathbf{y} | \\mathbf{a}) $$\n",
    "\n",
    "where...\n",
    "- The inequality $ L_s \\leq \\log p(\\mathbf{y} \\mid \\mathbf{a}) $ indicates that $ L_s $ is a lower bound on the log-likelihood. Lower bound means it is always less than or equal to the true log probability of the caption given the image.\n",
    "- The objective function $ L_s $ involves summing over all possible attention sequences, but since we can't compute this exactly, we use a weighted sum where the weights are the probabilities of each attention sequence: $ p(s | \\mathbf{a}) $.\n",
    "\n",
    "**Significance**: $ L_s $ serves as a computationally feasible approximation to the true log-likelihood of generating the correct caption. It is about finding the best possible set of attention decisions (where to focus in the image at each step) to maximize the probability of correctly generating the caption sequence. It's optimized during training to improve captioning accuracy.\n",
    "\n",
    "#### 5. Gradient Approximation via Monte Carlo Sampling\n",
    "\n",
    "$$ \\frac{\\partial L_s}{\\partial W} \\approx \\frac{1}{N} \\sum_{n=1}^N\\left[\\frac{\\partial \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a})}{\\partial W} + \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a}) \\frac{\\partial \\log p(\\tilde{s}^n | \\mathbf{a})}{\\partial W}\\right] $$\n",
    "\n",
    "where...\n",
    "- The gradient of $ L_s $ is approximated as an average over $ N $ sampled sequences of attention decisions.\n",
    "- $ \\frac{\\partial \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a})}{\\partial W} $ is the gradient of the log likelihood of the generated word sequence given the sampled attention sequence and the image features\n",
    "- $ \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a}) $ is the log likelihood of the word sequence given the sampled attention sequence and the image features\n",
    "- $ \\frac{\\partial \\log p(\\tilde{s}^n | \\mathbf{a})}{\\partial W} $ is the gradient of the log probability of the sampled attention sequence given the image features\n",
    "\n",
    "\n",
    "**Significance**: Provides a practical method to approximate the gradient of $ L_s $ for model optimization, as direct computation is infeasible due to the stochastic nature of hard attention.\n",
    "\n",
    "#### 6. Variance Reduction Techniques\n",
    "\n",
    "**Moving Average Baseline**\n",
    "$$ b_k = 0.9 \\times b_{k-1} + 0.1 \\times \\log p(\\mathbf{y} | \\tilde{s}_k, \\mathbf{a}) $$\n",
    "\n",
    "where...\n",
    "- $ b_k $ represents the moving average baseline at the $ k $-th mini-batch during training.\n",
    "- The formula for $ b_k $ involves an exponential decay component, which is a method commonly used to calculate a moving average that gives more weight to recent observations. In this case, the decay is controlled by the coefficient $ 0.9 $. This coefficient multiplies the previous baseline $ b_{k-1} $, effectively reducing its influence over time.\n",
    "\n",
    "\n",
    "- **Significance**: Reduces the variance in the Monte Carlo estimator of the gradient, stabilizing training.\n",
    "\n",
    "\n",
    "***Entropy Regularization***\n",
    "$$ \\lambda_e \\frac{\\partial H[\\tilde{s}^n]}{\\partial W} $$\n",
    "\n",
    "where...\n",
    "- $ H[\\tilde{s}^n] $ is the entropy of the sampled attention sequence $ \\tilde{s}^n $. By adding the entropy of the attention distribution to the objective function, the model is encouraged to maintain a degree of uncertainty in its attention decisions. This encouragement for higher entropy effectively promotes exploration in the model's attention mechanism. Instead of always focusing on the same regions for similar images or features, the model is nudged to explore other potentially informative regions as well.\n",
    "- $ \\lambda_e $ is a hyperparameter controlling the strength of the entropy regularization.\n",
    "\n",
    "- **Significance**: Encourages exploration in attention decisions, further reducing variance and improving model robustness. A model that explores more diverse attention strategies is less likely to get stuck in local optima and can generalize better.\n",
    "\n",
    "#### 7. Final Learning Rule\n",
    "\n",
    "$$ \\frac{\\partial L_s}{\\partial W} \\approx \\frac{1}{N} \\sum_{n=1}^N\\left[\\frac{\\partial \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a})}{\\partial W} + \\lambda_r(\\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a}) - b) \\frac{\\partial \\log p(\\tilde{s}^n | \\mathbf{a})}{\\partial W} + \\lambda_e \\frac{\\partial H[\\tilde{s}^n]}{\\partial W}\\right] $$\n",
    "\n",
    "where...\n",
    " \n",
    "- $ \\frac{\\partial \\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a})}{\\partial W} $ is the gradient of the objective function $ L_s $ with respect to the model parameters $ W $\n",
    "- $ \\lambda_r(\\log p(\\mathbf{y} | \\tilde{s}^n, \\mathbf{a}) - b) \\frac{\\partial \\log p(\\tilde{s}^n | \\mathbf{a})}{\\partial W} $ resembles the REINFORCE learning rule from reinforcement learning\n",
    "- $ \\lambda_r $ is a hyperparameter that controls the influence of the reinforcement learning-based reward signal in the training process. It adjusts the balance between following the gradient of the attention model’s log probability and the reinforcement learning-based reward signal.\n",
    "- $ \\lambda_e $ is a hyperparameter controlling the strength of the entropy regularization.\n",
    "\n",
    "​\n",
    "\n",
    "**Significance**: Combines all elements (gradient approximation, baseline, and entropy regularization) into a single learning rule for training the model with hard attention.\n",
    "\n",
    "\n",
    "\n",
    "#### Connection to REINFORCE Learning Rule\n",
    "\n",
    "This approach aligns with the REINFORCE rule from reinforcement learning, treating the sequence of attention decisions as actions with associated rewards based on the log likelihood of the generated caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to the implementation\n",
    "\n",
    "In an effort to make the implementation more closely align with the paper, I made the following changes to the base implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO-RNN according to the paper\n",
    "\n",
    "As we saw above, the paper describes the deep-output RNN as having multiple layers, each followed by a non-linear activation function. The implementation by AaronCCWong only had one layer transforming the hidden state of the LSTM. Therfore, I implemented the deep-output RNN as described in the paper, with multiple layers and non-linear activations. This can be enabled by using the `--ado` flag when training the model.\n",
    "\n",
    "$$ p\\left(\\mathbf{y}_t \\mid \\mathbf{a}, \\mathbf{y}_1^{t-1}\\right) \\propto \\exp \\left(\\mathbf{L}_o\\left(\\mathbf{E} \\mathbf{y}_{t-1}+\\mathbf{L}_h \\mathbf{h}_t+\\mathbf{L}_z \\hat{\\mathbf{z}}_t\\right)\\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ \\mathbf{L}_o, \\mathbf{L}_h, \\mathbf{L}_z $ are learned weight matrices for transforming the embedding, hidden state, and context vector respectively.\n",
    "- $ \\exp ( ) $ represents the softmax function, which is automatically applied by the CrossEntropyLoss function in PyTorch (TODO: verify this).\n",
    "\n",
    "```python\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocabulary_size, encoder_dim, tf=False, ado=False):\n",
    "        # ...\n",
    "        # Simple DO: Layer for transforming LSTM state to vocabulary\n",
    "        self.deep_output = nn.Linear(512, vocabulary_size)  # Maps LSTM outputs to vocabulary\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        # Advanced DO: Layers for transforming LSTM state, context vector and embedding for DO-RNN\n",
    "        hidden_dim, intermediate_dim = 512, 512\n",
    "        self.f_h = nn.Linear(hidden_dim, intermediate_dim)  # Transforms LSTM hidden state\n",
    "        self.f_z = nn.Linear(encoder_dim, intermediate_dim)  # Transforms context vector\n",
    "        self.f_out = nn.Linear(intermediate_dim, vocabulary_size)  # Transforms the combined vector (sum of embedding, LSTM state, and context vector) to vocabulary\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.dropout = nn.Dropout()\n",
    "        # ...\n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        # ...\n",
    "        for t in range(max_timespan):\n",
    "            # ...\n",
    "            # Generate word prediction\n",
    "            if self.use_advanced_deep_output:\n",
    "                output = self.advanced_deep_output(self.dropout(h), context, captions, embedding, t)\n",
    "            else:\n",
    "                output = self.deep_output(self.dropout(h))\n",
    "            # ...\n",
    "    \n",
    "    def advanced_deep_output(self, h, context, captions, embedding, t):\n",
    "        # Combine the LSTM state and context vector\n",
    "        h_transformed = self.relu(self.f_h(h))\n",
    "        z_transformed = self.relu(self.f_z(context))\n",
    "\n",
    "        # Sum the transformed vectors with the embedding\n",
    "        combined = h_transformed + z_transformed + self.embedding(captions[:, t] if self.training else embedding)\n",
    "\n",
    "        # Transform the combined vector & compute the output word probability\n",
    "        return self.relu(self.f_out(combined))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- [ ] Verify that the softmax function is applied by the CrossEntropyLoss function in PyTorch.\n",
    "- [ ] Hard vs. Soft attention, which one is implemented here?\n",
    "- [ ] Try pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
