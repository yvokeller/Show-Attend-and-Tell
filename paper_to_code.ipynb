{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "My goal with this notebook is to show the direct relation between the paper (Show, Attend and Tell) and it's implementation within this repository. I will try to explain the code as much as possible, nontheless recommend you to read the paper first.\n",
    "\n",
    "The implementation is based on the [AaronCCWong's implementation](https://github.com/AaronCCWong/Show-Attend-and-Tell). I build on top of his work, trying some alternative techniques (like using pre-trained embeddings) and more closely aligning it with the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3.1.1 in the \"Show, Attend and Tell\" paper describes the encoder. The proposed model uses a CNN to extract a set of feature vectors, referred to as annotation vectors. This code implements that concept by using pre-trained models (VGG19, ResNet152, or DenseNet161), modifying them to exclude the final classification layers, and reshaping the output to form a set of feature vectors (our attention vectors). Each vector represents different spacial parts of the image, which is key for the attention mechanism in the next stages of the model.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from torchvision.models import densenet161, resnet152, vgg19\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network for image feature extraction, follows section 3.1.1 of the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, network='vgg19'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.network = network\n",
    "        # Selection of pre-trained CNNs for feature extraction\n",
    "        if network == 'resnet152':\n",
    "            self.net = resnet152(pretrained=True)\n",
    "            # Removing the final fully connected layers of ResNet152\n",
    "            self.net = nn.Sequential(*list(self.net.children())[:-2])\n",
    "            self.dim = 2048  # Dimension of feature vectors for ResNet152\n",
    "        elif network == 'densenet161':\n",
    "            self.net = densenet161(pretrained=True)\n",
    "            # Removing the final layers of DenseNet161\n",
    "            self.net = nn.Sequential(*list(list(self.net.children())[0])[:-1])\n",
    "            self.dim = 1920  # Dimension of feature vectors for DenseNet161\n",
    "        else:\n",
    "            self.net = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "            # Using features from VGG19, excluding the last pooling layer\n",
    "            self.net = nn.Sequential(*list(self.net.features.children())[:-1])\n",
    "            self.dim = 512  # Dimension of feature vectors for VGG19\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # These steps correspond to the extraction of annotation vectors (a = {a1,...,aL}) as described in Section 3.1.1 of the paper.\n",
    "        # 1. Change the order from (BS, C, H, W) to (BS, H, W, C) in prep for reshaping\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # 2. Reshape to [BS, num_spatial_features, C], the -1 effectively flattens the height and width dimensions into a single dimension\n",
    "        x = x.view(x.size(0), -1, x.size(-1))\n",
    "        return x\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "\n",
    "Let's move forward to Section 3.1.2, explaining the Decoder. The Decoder uses the extracted image features to initialize the states of the LSTM cell (by averaging them) and then employs an **attention mechanism** at each time step to focus on different parts of the image while generating the caption. The Decoder predicts one word of the caption at each time step, and its prediction is conditioned on the current LSTM state, the context vector from the attention mechanism, and the previous word. Teacher forcing, a common technique in training sequence generation models where the ground truth word is fed as the next input instead of the model's prediction, is optionally used.\n",
    "\n",
    "Provided below is the full implementation of the Decoder. I will break the code down into smaller chunks to explore it in more detail, based on the paper's description of the Decoder.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import Attention\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocabulary_size, encoder_dim, tf=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_tf = tf\n",
    "\n",
    "        # Initializing parameters\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "        # Initial LSTM cell state generators\n",
    "        self.init_h = nn.Linear(encoder_dim, 512)  # For hidden state\n",
    "        self.init_c = nn.Linear(encoder_dim, 512)  # For cell state\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Attention mechanism related layers\n",
    "        self.f_beta = nn.Linear(512, encoder_dim)  # Gating scalar in attention mechanism\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Output layer and embedding\n",
    "        self.deep_output = nn.Linear(512, vocabulary_size)  # Maps LSTM outputs to vocabulary\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        # Attention and LSTM components\n",
    "        self.attention = Attention(encoder_dim)  # Attention network\n",
    "        self.embedding = nn.Embedding(vocabulary_size, 512)  # Embedding layer for input words\n",
    "        self.lstm = nn.LSTMCell(512 + encoder_dim, 512)  # LSTM cell\n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        # Forward pass of the decoder\n",
    "        batch_size = img_features.size(0)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.get_init_lstm_state(img_features)\n",
    "\n",
    "        # Teacher forcing setup\n",
    "        max_timespan = max([len(caption) for caption in captions]) - 1\n",
    "        prev_words = torch.zeros(batch_size, 1).long().to(mps_device)\n",
    "        if self.use_tf:\n",
    "            embedding = self.embedding(captions) if self.training else self.embedding(prev_words)\n",
    "        else:\n",
    "            embedding = self.embedding(prev_words)\n",
    "\n",
    "        # Preparing to store predictions and attention weights\n",
    "        preds = torch.zeros(batch_size, max_timespan, self.vocabulary_size).to(mps_device)\n",
    "        alphas = torch.zeros(batch_size, max_timespan, img_features.size(1)).to(mps_device)\n",
    "\n",
    "        # Generating captions\n",
    "        for t in range(max_timespan):\n",
    "            context, alpha = self.attention(img_features, h)  # Compute context vector via attention\n",
    "            gate = self.sigmoid(self.f_beta(h))  # Gating scalar for context\n",
    "            gated_context = gate * context  # Apply gate to context\n",
    "\n",
    "            # Prepare LSTM input\n",
    "            if self.use_tf and self.training:\n",
    "                lstm_input = torch.cat((embedding[:, t], gated_context), dim=1)\n",
    "            else:\n",
    "                embedding = embedding.squeeze(1) if embedding.dim() == 3 else embedding\n",
    "                lstm_input = torch.cat((embedding, gated_context), dim=1)\n",
    "\n",
    "            # LSTM forward pass\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            output = self.deep_output(self.dropout(h))  # Generate word prediction\n",
    "\n",
    "            preds[:, t] = output\n",
    "            alphas[:, t] = alpha  # Store attention weights\n",
    "\n",
    "            # Prepare next input word\n",
    "            if not self.training or not self.use_tf:\n",
    "                embedding = self.embedding(output.max(1)[1].reshape(batch_size, 1))\n",
    "        return preds, alphas\n",
    "\n",
    "    def get_init_lstm_state(self, img_features):\n",
    "        # Initializing LSTM state based on image features\n",
    "        avg_features = img_features.mean(dim=1)\n",
    "\n",
    "        c = self.init_c(avg_features)  # Cell state\n",
    "        c = self.tanh(c)\n",
    "\n",
    "        h = self.init_h(avg_features)  # Hidden state\n",
    "        h = self.tanh(h)\n",
    "\n",
    "        return h, c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The deep_output layer\n",
    "\n",
    "Show, Attend and Tell utilizes a *deep output layer* (Pascanu et al., 2014) to compute the output word probability given given the current state of the LSTM, the context vector from the attention mechanism, and the previously generated word.\n",
    "\n",
    "Let's break down this formula and map its components to the code:\n",
    "\n",
    "$$ p\\left(\\mathbf{y}_t \\mid \\mathbf{a}, \\mathbf{y}_1^{t-1}\\right) \\propto \\exp \\left(\\mathbf{L}_o\\left(\\mathbf{E} \\mathbf{y}_{t-1}+\\mathbf{L}_h \\mathbf{h}_t+\\mathbf{L}_z \\hat{\\mathbf{z}}_t\\right)\\right) $$\n",
    "\n",
    "\n",
    "\n",
    "Where p of $ \\mathbf{y}_t $ is the probability of the output word at time $ t $ given the image features $ \\mathbf{a} $ and the previously generated words $ \\mathbf{y}_1^{t-1} $.\n",
    "\n",
    "In this formula:\n",
    "\n",
    "- $ \\mathbf{y}_t $ is the output word at time $ t $.\n",
    "- $ \\mathbf{a} $ represents the set of annotation vectors (image features).\n",
    "- $ \\mathbf{y}_1^{t-1} $ are the previously generated words up to time $ t-1 $.\n",
    "- $ \\mathbf{L}_o, \\mathbf{L}_h, \\mathbf{L}_z $ are learned weight matrices.\n",
    "- $ \\mathbf{E} $ is the embedding matrix for the previous word $ \\mathbf{y}_{t-1} $.\n",
    "- $ \\mathbf{h}_t $ is the hidden state of the LSTM at time $ t $.\n",
    "- $ \\hat{\\mathbf{z}}_t $ is the context vector at time $ t $, generated by the attention mechanism.\n",
    "\n",
    "Now let's map this to the Decoder's code:\n",
    "\n",
    "1. **Embedding of the Previous Word ($ \\mathbf{E} \\mathbf{y}_{t-1} $)**: This is done using the `self.embedding` layer in the code.\n",
    "\n",
    "    ```python\n",
    "    embedding = self.embedding(prev_words)\n",
    "    ```\n",
    "\n",
    "2. **Hidden State of the LSTM ($ \\mathbf{h}_t $)**: The `h` variable in the code represents the hidden state of the LSTM at each time step.\n",
    "\n",
    "    ```python\n",
    "    h, c = self.lstm(lstm_input, (h, c))\n",
    "    ```\n",
    "\n",
    "3. **Context Vector ($ \\hat{\\mathbf{z}}_t $)**: The context vector is computed by the attention mechanism in the `self.attention` layer.\n",
    "\n",
    "    ```python\n",
    "    context, alpha = self.attention(img_features, h)\n",
    "    ```\n",
    "\n",
    "4. **Combining and Transforming for Output Prediction**: The output word probability is computed by combining these elements and applying the learned weight matrices. In the code, this operation is currently condensed into one `self.deep_output` layer transforming the hidden state $ \\mathbf{h}_t $. In a more complex or literal implementation of the DO-RNN, you would expect to see multiple such layers, each followed by a non-linear activation function.\n",
    "\n",
    "    ```python\n",
    "    output = self.deep_output(self.dropout(h))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to the implementation\n",
    "\n",
    "In an effort to make the implementation more closely align with the paper, I made the following changes to the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO-RNN according to the paper\n",
    "\n",
    "As we saw above, the paper describes the DO-RNN as having multiple layers, each followed by a non-linear activation function. The original implementation only has one layer, transforming the hidden state of the LSTM.\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
